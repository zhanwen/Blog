---
title: lucene笔记
date: 2017-11-31 22:43:24
tags:
	- Lucene
	- 搜索
	- 技术
---
![Alt text](/images/lucene.jpg)
# 什么是全文检索
&emsp;&emsp;全文检索是计算机程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的次数和位置。当用户查询时根据建立的索引查找，类似于通过字典的检索字表查字的过程。
&emsp;&emsp;全文检索（Full-Text Retrieval）是指以文本作为检索对象，找出含有指定词汇的文本。全面、准确和快速是衡量全文检索系统的关键指标。
关于全文检索
	
	1、只处理文本
	2、不处理语义
	3、搜索时英文不区分大小写
	4、结果列表有相关度排序
在信息检索工具中，全文检索是最具通用性和实用性的。
<!-- more -->

# 全文检索应用场景
&emsp;&emsp;我们使用Lucene，主要是做站内搜索，即对一个系统内的资源进行搜索。如BBS、Blog中的文章搜索，网上商店中的商品搜索等。使用Lucene的项目有Eclipse,智联招聘,天猫等。一般不做互联网中资源的搜索，因为不易获取与管理海量资源（专业搜索方向的公司除外）。

全文检索不同于数据库检索
&emsp;&emsp;全文检索不同于数据库的SQL查询.(他们所解决的问题不一样，解决的方案也不一样，所以不应进行对比）。在数据库中的搜索就是使用SQL。
	
	SELECT * FROM t WHERE content like ‘%ant%’

这样会有如下问题：
	1、匹配效果：如搜索ant会搜索出planting。这样就会搜出很多无关的信息。
	2、相关度排序：查出的结果没有相关度排序，不知道我想要的结果在哪一页。我们在使用百度搜索时，一般不需要翻页，为什么？因为百度做了相关度排序：为每一条结果打一个分数，这条结果越符合搜索条件，得分就越高，叫做相关度得分，结果列表会按照这个分数由高到低排列，所以第1页的结果就是我们最想要的结果。
	3、全文检索的速度大大快于SQL的like搜索的速度。这是因为查询方式不同造成的，以查字典举例：数据库的like就是一页一页的翻，一行一行的找，而全文检索是先查目录，得到结果所在的页码，再直接翻到这一页。

关于lucene索引库操作的更新
	1、索引文件的检索与维护，更新是先删除后创建。
	2、维护倒排索引有三个操作：添加、删除和更新文档。但是更新操作需要较高的代价。因为文档修改后（即使是很小的修改），就可能会造成文档中的很多的关键词的位置都发生了变化，这就需要频繁的读取和修改记录，这种代价是相当高的。因此，一般不进行真正的更新操作，而是使用`先删除，再创建`的方式代替更新操作。

# 索引库的优化
&emsp;&emsp;优化这个问题是比较纠结的，索引优化也是很费资源和时间的，但是优化索引也是提高检索速度的重要方法，因此需要好好权衡这一点。还有就是在lucene3.6后面的版本中lucene可以自动进行索引的优化，当索引的数目达到一定的量之后会自动进行索引的优化。具体的优化方法有以下几种
	1、声明Directory对象
    2、声明IndexWriter对象
    3、执行优化的方法，参数表示优化称几段索引
    4、关闭IndexWriter

# 分词器
分词器的作用
&emsp;&emsp;在创建索引时会用到分词器，在使用字符串搜索时也会用到分词器，这两个地方要使用同一个分词器，否则可能会搜索不出结果。
&emsp;&emsp;Analyzer（分词器）的作用是把一段文本中的词按规则取出所包含的所有词。对应的是Analyzer类，这是一个抽象类，切分词的具体规则是由子类实现的，所以对于不同的语言（规则），要用不同的分词器。
&emsp;&emsp;分词器的工作流程如下
	
	(英文) 分词器：
	1、切分关键词
	2、去除停用词 
	3、对于英文单词，把所有字母转为小写（搜索时不区分大小写）
说明：有的分词器还对英文进行形态还原，就是去除单词词尾的形态变化，将其还原为词的原形。这样做可以搜索出更多有意义的结果。如搜索sutdent时，也可以搜索出students，这是很有用的。

关于分词器中的停用词和切分词
&emsp;&emsp;有些词在文本中出现的频率非常高，但是对文本所携带的信息基本不产生影响，例如英文的“a、an、the、of”，或中文的“的、了、着、是”，以及各种标点符号等，这样的词称为停用词（stop word）。文本经过分词之后，停用词通常被过滤掉，不会被进行索引。在检索的时候，用户的查询中如果含有停用词，检索系统也会将其过滤掉（因为用户输入的查询字符串也要进行分词处理）。排除停用词可以加快建立索引的速度，减小索引库文件的大小。

常用的中文分词器
&emsp;&emsp;中文的分词比较复杂，因为不是一个字就是一个词，而且一个词在另外一个地方就可能不是一个词，如在“帽子和服装”中，“和服”就不是一个词。对于中文分词，通常有三种方式：单字分词、二分法分词、词库分词。
     
     1、单字分词：就是按照中文一个字一个字地进行分词。如：“我们是中国人”， 效果：“我”、“们”、“是”、“中”、“国”、“人”。（StandardAnalyzer、ChineseAnalyzer就是这样）。
     2、二分法分词：按两个字进行切分。如：“我们是中国人”，效果：“我们”、“们是”、“是中”、“中国”、“国人”。（CJKAnalyzer就是这样）。
     3、词库分词：按某种算法构造词，然后去匹配已建好的词库集合，如果匹配到就切分出来成为词语。通常词库分词被认为是最理想的中文分词算法。如：“我们是中国人”，效果为：“我们”、“中国人”。（使用极易分词的MMAnalyzer。可以使用“极易分词”，或者是“庖丁分词”分词器、IKAnalyzer）。
















